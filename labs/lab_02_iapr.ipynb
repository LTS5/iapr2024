{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4BXjF6YIa1t"
   },
   "source": [
    "# [IAPR][iapr]: Lab 2 â€’  Object description\n",
    "\n",
    "\n",
    "**Group ID:** xx\n",
    "\n",
    "**Author 1 (sciper):** Student Name 1 (xxxxx)  \n",
    "**Author 2 (sciper):** Student Name 2 (xxxxx)   \n",
    "**Author 3 (sciper):** Student Name 3 (xxxxx)   \n",
    "\n",
    "**Release date:** 22.03.2024  \n",
    "**Due date:** 19.04.2024 (11:59 pm)\n",
    "\n",
    "\n",
    "## Important notes\n",
    "\n",
    "The lab assignments are designed to teach practical implementation of the topics presented during class as well as preparation for the final project, which is a practical project that ties together the topics of the course.\n",
    "\n",
    "As such, in the lab assignments/final project, unless otherwise specified, you may, if you choose, use external\n",
    "functions from image processing/ML libraries like OpenCV and sklearn as long as there is sufficient explanation\n",
    "in the lab report. For example, you do not need to implement your own edge detector, etc.\n",
    "\n",
    "**! Before handling back the notebook <font color='red'> rerun </font>the notebook from scratch !**\n",
    "`Kernel` > `Restart & Run All`\n",
    "\n",
    "TAs should be able to rerun your code end to end without having any issues. If not, you might lose part of the points during grading.\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wget\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install scikit-image\n",
    "!pip install scikit-learn\n",
    "!pip install python-mnist\n",
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check is at least python 3.9\n",
    "import sys \n",
    "assert (sys.version_info.major == 3) and (sys.version_info.minor == 9)\n",
    "# Other global libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import wget\n",
    "import os\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Introduction\n",
    "\n",
    "In this lab, we will work with the famous MNIST dataset. It is composed of thousands of images (size 28x28) that depict handwritten digits from 0 to 9. The code below will automatically download the data from the online repo (http://yann.lecun.com/exdb/mnist/). This lab aims to create discriminant features from handwritten digits using various approaches. \n",
    "\n",
    "\n",
    "Take a look at the data to get a better idea of what you will be working within this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a local folder\n",
    "folder_lab = os.path.join(\"..\", \"data\", \"data_lab_02\")\n",
    "os.makedirs(folder_lab, exist_ok=True)\n",
    "# Data url\n",
    "url_img = \"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\"\n",
    "url_label = \"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\"\n",
    "\n",
    "# Data filename \n",
    "file_img = os.path.join(folder_lab, \"train-images-idx3-ubyte.gz\")\n",
    "file_label = os.path.join(folder_lab, \"train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# Download data \n",
    "if not os.path.exists(file_img):\n",
    "    file_img = wget.download(url_img, out=file_img)\n",
    "    file_label = wget.download(url_label, out=file_label)\n",
    "\n",
    "print(\"Data downloaded under folder: {}\".format(folder_lab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mnist import MNIST\n",
    "\n",
    "# Load data\n",
    "mndata = MNIST(folder_lab, gz=True)\n",
    "images, labels = mndata.load_training()\n",
    "\n",
    "# Convert as numpy arrays and binarize images\n",
    "images = np.array(images).reshape((-1, 28, 28))\n",
    "labels = np.array(labels)\n",
    "\n",
    "print(\"{} Images and {} labels loaded\".format(len(images), len(labels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(images: np.ndarray, labels:np.ndarray, title: str, cnt: list = None):\n",
    "    \"\"\"\n",
    "    Display images along with labels. \n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "    labels: np.ndarray (N)\n",
    "        List of labels associated with the input image\n",
    "    title: str\n",
    "        Title of the plot\n",
    "    cnt: list\n",
    "        List of contours to display (only used for exercise 1.3 and more)\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of images, columns, and rows\n",
    "    n = len(images)\n",
    "    n_cols = 8\n",
    "    r_rows = np.ceil(n/n_cols).astype(int)\n",
    "    \n",
    "    # Define plot\n",
    "    _, axes = plt.subplots(r_rows, n_cols, figsize=(14, 2*r_rows))\n",
    "    axes = axes.ravel()\n",
    "    \n",
    "    \n",
    "    # Plot all images and labels\n",
    "    for i in range(n):\n",
    "        axes[i].imshow(images[i], interpolation=\"nearest\")\n",
    "        axes[i].axis(\"off\")\n",
    "        axes[i].set_title(labels[i])\n",
    "        # Check if need to display contour\n",
    "        if cnt is not None and len(cnt) == n:\n",
    "            axes[i].plot(cnt[i][:, 0], cnt[i][:, 1], 'r-*')\n",
    "\n",
    "    # Set title\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "        \n",
    "display_samples(images[:24], labels[:24], title=\"First 24 samples with labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1 - Preprocessing\n",
    "\n",
    "\n",
    "In this lab, we will create different feautre descriptors from digits. However, we will not use all images in the dataset. We will focus on the digits \"0\" and \"4\".\n",
    "\n",
    "## 1.1 Selection (1 pts)\n",
    "\n",
    "**Q1 (1 pts)** Your first task is to complete the function `extract_label` such that it selects from the input data only the images that are labeled as a given `target_label`. This function will be used to extract 0s and 4s from the data cohort. When running the code, the plots should only show you samples that are 0s (first plot) and 4s (second plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label(images: np.ndarray, labels: np.ndarray, target_label: int):\n",
    "    \"\"\"\n",
    "    The function returns only the images that have target_label as labels.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images - handwritten digits \n",
    "    labels: np.ndarray (N)\n",
    "        List of labels associated with the input image\n",
    "    target_label: int\n",
    "        Selected target label\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    img_extract: np.ndarray (M, 28, 28)\n",
    "        Extracted images that have target_label as label (M should be lower than N).\n",
    "    \"\"\"\n",
    "\n",
    "    n, d, _ = np.shape(images) \n",
    "    img_extract = np.zeros((30, d, d))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return img_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of digits to include\n",
    "n = 1000\n",
    "# Set the number of digits to display\n",
    "n_plot=24\n",
    "n_samples = 11\n",
    "# Set which digit to consider\n",
    "label_a = 0\n",
    "label_b = 4\n",
    "\n",
    "# Filter images\n",
    "images_a = extract_label(images, labels=labels, target_label=label_a)[:n]\n",
    "images_b = extract_label(images, labels=labels, target_label=label_b)[:n]\n",
    "\n",
    "# Display random results\n",
    "display_samples(images=images_a[:n_plot], labels=[str(label_a)]*n_plot, title=\"Selected {}s (n={})\".format(label_a, n_plot))\n",
    "display_samples(images=images_b[:n_plot], labels=[str(label_b)]*n_plot, title=\"Selected {}s (n={})\".format(label_b, n_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Preprocessing (1 pts)\n",
    "\n",
    "Before computing the Fourier descriptors we need to preprocess the images.\n",
    "\n",
    "* **Q1 (1 pts)**: Complete the function `preprocess` such that it cleans the input images. Take a look at the example images above and try to think what could be improved to allow better uniformity of the data. Take advantage of what you have learned in the previous lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(images: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply the processing step to images to achieve better data uniformity.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    img_process: np.ndarray (N, 28, 28)\n",
    "        Processed images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the shape of input data and set dummy values\n",
    "    n, d, _ = np.shape(images) \n",
    "    img_process = np.zeros_like(images)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return img_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract images with 0s ans 4s\n",
    "images_p_a = preprocess(images=images_a)\n",
    "images_p_b = preprocess(images=images_b)\n",
    "\n",
    "# Display results\n",
    "display_samples(images=images_p_a[:n_plot], labels=[label_a]*n_plot, title=\"Selected {}s (n={})\".format(label_a, n_plot))\n",
    "display_samples(images=images_p_b[:n_plot], labels=[label_b]*n_plot, title=\"Selected {}s (n={})\".format(label_b, n_plot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2 - Fourier Descriptors\n",
    "\n",
    "\n",
    "## 2.1. Get contour and descriptors (9 pts)\n",
    "\n",
    "In this section, we will create Fourier descriptors from digits. The next step in our pipeline will be to detect the contours. To do so you can use existing algorithm available online such as `find_contours`([doc](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.find_contours)) from skcit-image or `findContours` ([doc](https://docs.opencv.org/4.x/d3/dc0/group__imgproc__shape.html#gadf1ad6a0b82947fa1fe3c3d497f260e0)) from opencv. \n",
    "\n",
    "* **Q1 (2 pts)**: Complete the function `find_contour` below such that it returns the contour estimations of the given images. The provided `display_samples` function will display the returned contours for a subset of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_contour(images: np.ndarray):\n",
    "    \"\"\"\n",
    "    Find the contours for the set of images\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    images: np.ndarray (N, 28, 28)\n",
    "        Source images to process\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    \"\"\"\n",
    "\n",
    "    # Get number of images to process\n",
    "    N, _, _ = np.shape(images)\n",
    "    # Fill in dummy values (fake points)\n",
    "    contours = [np.array([[0, 0], [1, 1]]) for i in range(N)]\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return contours\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get contours\n",
    "cnt_p_a = find_contour(images_p_a)\n",
    "cnt_p_b = find_contour(images_p_b)\n",
    "\n",
    "# Define plot titles\n",
    "title_a = \"Preprocessed {}s w/ contours (n={})\".format(label_a, n_plot)\n",
    "title_b = \"Preprocessed {}s w/ contours (n={})\".format(label_b, n_plot)\n",
    "\n",
    "# Display results\n",
    "display_samples(\n",
    "    images=images_p_a[:n_plot], labels=[label_a]*n_plot, cnt=cnt_p_a[:n_plot], title=title_a)\n",
    "display_samples(\n",
    "    images=images_p_b[:n_plot], labels=[label_b]*n_plot, cnt=cnt_p_b[:n_plot], title=title_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now that we are able to properly detect shape contours, we can finally compute Fourier descriptors. However, we still face a small issue. The allow a fair comparison between the Fourier descriptors we need to ensure that all descriptors have the same length.\n",
    "\n",
    "* **Q2 (2 pts)**: Implement the function `compute_descriptor_padding` that takes as input the computed contours from before and returns the Fourier descriptors for each contour. Use `fft` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.fft.fft.html#numpy.fft.fft)) from Numpy to compute the transformation. Use the `n_sample` argument to set the number of points to consider per contour. If the contour is longer than `n_sample` discard the extra points. If the contour is shorter than `n_sample`, use 0 paddings. Make sure that the first element of the descriptor represent the continuous component in the frequency domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptor_padding(contours: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Compute Fourier descriptors of input images\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider. If the contour length is higher, discard the remaining part. If it is shorter, add padding.\n",
    "        Make sure that the first element of the descriptor represents the continuous component.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    descriptors: np.ndarray complex (N, n_samples)\n",
    "        Computed complex Fourier descriptors for the given input images\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    # Look for the number of contours\n",
    "    descriptors = np.zeros((N, n_samples), dtype=np.complex_)\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return descriptors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uses the `plot_features` function to display the computed Fourier descriptors. We display the real, imaginary, and absolute components, respectively. Each color depicts a different digit. We use 2D plots to highlight the clustering capability of the components.\n",
    "* **Q3 (1 pts)**: Comment on the quality of the Fourier descriptors. Do you think they are good feature descriptors? (justify)\n",
    "    * **Answer**: ...\n",
    "* **Q4 (1 pts)**: Knowing that we used `n_samples=11` to compute the Fourier transform, what do the components 0, 1, 5, and 10 represent as frequencies? (high, medium, constant, etc.)\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(features_a: np.ndarray, features_b: np.ndarray, label_a: str, label_b: str, title: str):\n",
    "    \"\"\"\n",
    "    Plot feature components a and b.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    features_a: np.ndarray (N, D)\n",
    "        Feature a with N samples and D complex features. \n",
    "    features_b: np.ndarray (N, D)\n",
    "        Feature b with N samples and D complex features.\n",
    "    label_a: str\n",
    "        Name of the feature a.\n",
    "    label_b: str\n",
    "        Name of the feature b.\n",
    "    \"\"\"\n",
    "\n",
    "    # Number of paris to display\n",
    "    n_features = features_a.shape[1]\n",
    "    # Define pairs for 2D plots\n",
    "    pairs = np.array(range(2*np.ceil(n_features / 2).astype(int)))\n",
    "    # Check if odd lenght, shift second feature to have pairs\n",
    "    if n_features % 2 == 1:\n",
    "        pairs[2:] = pairs[1:-1]\n",
    "    # Convert to 2d array\n",
    "    pairs = pairs.reshape(-1, 2)\n",
    "\n",
    "    # Plot each pairs and labels\n",
    "    n_plots = len(pairs)\n",
    "    _, axes = plt.subplots(3, n_plots, figsize=(15, 8))\n",
    "    \n",
    "    for i, (pa, pb) in enumerate(pairs):\n",
    "        # Real\n",
    "        axes[0, i].scatter(np.real(features_a[:, pa]), np.real(features_a[:, pb]), label=label_a, s=10, alpha=0.1)\n",
    "        axes[0, i].scatter(np.real(features_b[:, pa]), np.real(features_b[:, pb]), label=label_b, s=10, alpha=0.1)\n",
    "        axes[0, i].set_xlabel(\"Component {}\".format(pa))\n",
    "        axes[0, i].set_ylabel(\"Component {}\".format(pb))\n",
    "        axes[0, i].set_title(\"Real {} vs {}\".format(pa, pb))\n",
    "        axes[0, i].legend()\n",
    "        # Imag\n",
    "        axes[1, i].scatter(np.imag(features_a[:, pa]), np.imag(features_a[:, pb]), label=label_a, s=10, alpha=0.1)\n",
    "        axes[1, i].scatter(np.imag(features_b[:, pa]), np.imag(features_b[:, pb]), label=label_b, s=10, alpha=0.1)\n",
    "        axes[1, i].set_xlabel(\"Component {}\".format(pa))\n",
    "        axes[1, i].set_ylabel(\"Component {}\".format(pb))\n",
    "        axes[1, i].set_title(\"Imag. {} vs {}\".format(pa, pb))\n",
    "        axes[1, i].legend()\n",
    "        # Abs\n",
    "        axes[2, i].scatter(np.abs(features_a[:, pa]), np.abs(features_a[:, pb]), label=label_a, s=10, alpha=0.1)\n",
    "        axes[2, i].scatter(np.abs(features_b[:, pa]), np.abs(features_b[:, pb]), label=label_b, s=10, alpha=0.1)\n",
    "        axes[2, i].set_xlabel(\"Component {}\".format(pa))\n",
    "        axes[2, i].set_ylabel(\"Component {}\".format(pb))\n",
    "        axes[2, i].set_title(\"Abs. {} vs {}\".format(pa, pb))\n",
    "        axes[2, i].legend()\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the descriptors based on the contours\n",
    "feat_a = compute_descriptor_padding(contours=cnt_p_a, n_samples=n_samples)\n",
    "feat_b = compute_descriptor_padding(contours=cnt_p_b, n_samples=n_samples)\n",
    "\n",
    "# Plot components\n",
    "plot_features(\n",
    "    features_a=feat_a,\n",
    "    features_b=feat_b,\n",
    "    label_a=label_a, label_b=label_b,\n",
    "    title=\"Real/Imag./Absolute features\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might have realized before, the computed Fourier descriptors do not help us to find an optimal separation between the digits. This mainly comes from the fact that using 0 padding is a cheap and inefficient way to ensure homogeneity in length for descriptors. A better solution would be to ensure that we have the same number of points along the contour for each digit. To do so, we need to implement a new function that will resample the contour such that we always have the same number of points no matter the shape.\n",
    "\n",
    "* **Q5 (2 pts)**: Implement the function `linear_interpolation` that takes as input the contours with various lengths and the wanted number of samples per contour for resampling. For each contour, resample the points such that each contour has the same length `n_samples`. We want the points to be uniformly distributed (same distance between points) along the contour. You can use for example the function `interp` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.interp.html)) from Numpy to perform the interpolation. The function `display_samples` will display the contour for different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_interpolation(contours: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Perform interpolation/resampling of the contour across n_samples.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    contours: list of np.ndarray\n",
    "        List of N arrays containing the coordinates of the contour. Each element of the \n",
    "        list is an array of 2d coordinates (K, 2) where K depends on the number of elements \n",
    "        that form the contour. \n",
    "    n_samples: int\n",
    "        Number of samples to consider along the contour.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    contours_inter: np.ndarray complex (N, n_samples, 2)\n",
    "        Interpolated contour with n_samples\n",
    "    \"\"\"\n",
    "\n",
    "    N = len(contours)\n",
    "    contours_inter = np.zeros((N, n_samples, 2))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "        \n",
    "    return contours_inter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a different number of samples for the contour of the shapes\n",
    "n_samples_test = [2, 5, 10, 15, 20, 40, 60, 80]\n",
    "\n",
    "# Resample contours\n",
    "cs = []\n",
    "for i, n in enumerate(n_samples_test):\n",
    "    c = linear_interpolation(cnt_p_a, n_samples=n)\n",
    "    cs.append(c[0])\n",
    "\n",
    "# Display results with overlay\n",
    "display_samples(\n",
    "    images=np.repeat(images_p_a[0][None], repeats=8, axis=0), \n",
    "    labels=[\"n={}\".format(n) for n in n_samples_test], \n",
    "    cnt=cs, \n",
    "    title=\"Contour interpolation w/ different values\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our resampling approach we can revisualize the clustering efficiency of our descriptors.\n",
    "\n",
    "* **Q6 (1 pts)**: Comment on the quality of the descriptors. Is it better than before? is there a frequency/component that appears to work better? Does it make sense?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature descriptors with resampling\n",
    "feat_a = compute_descriptor_padding(contours=linear_interpolation(cnt_p_a, n_samples=n_samples), n_samples=n_samples)\n",
    "feat_b = compute_descriptor_padding(contours=linear_interpolation(cnt_p_b, n_samples=n_samples), n_samples=n_samples)\n",
    "\n",
    "# Plot components\n",
    "plot_features(\n",
    "    features_a=feat_a,\n",
    "    features_b=feat_b,\n",
    "    label_a=label_a, label_b=label_b,\n",
    "    title=\"Real/Imag./Absolute features (resampling)\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Reconstruction (2pts)\n",
    "\n",
    "For this part, we will now try to go the other way. Given a Fourier descriptor we will try to retrieve the original shape. \n",
    "* **Q1 (1 pts)** Implement the function `compute_reverse_descriptor` that takes as input a single descriptor and reverses it to x and y coordinates given a number of samples `n_samples`. Use the function `ifft` ([doc](https://numpy.org/doc/stable/reference/generated/numpy.fft.ifft.html)) from Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reverse_descriptor(descriptor: np.ndarray, n_samples: int = 11):\n",
    "    \"\"\"\n",
    "    Reverse a Fourier descriptor to xy coordinates given a number of samples.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    descriptor: np.ndarray (D,)\n",
    "        Complex descriptor of length D.\n",
    "    n_samples: int\n",
    "        Number of samples to consider to reverse transformation.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    x: np.ndarray complex (n_samples,)\n",
    "        x coordinates of the contour\n",
    "    y: np.ndarray complex (n_samples,)\n",
    "        y coordinates of the contour\n",
    "    \"\"\"\n",
    "\n",
    "    x = np.zeros(n_samples)\n",
    "    y = np.zeros(n_samples)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function `plot_reconstruction` we plot an example of the reconstruction of the digits. We display the result as we progressively add more frequencies. We start with the first component (component 0) and then add frequency pairs from low to high frequencies.\n",
    "\n",
    "* **Q2 (1 pts)**: Based on your observation, do you think 11 samples are enough to properly describe the digits below? (justify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstruction(image, descriptor):\n",
    "    \"\"\"\n",
    "    Plot Fourier descriptors reconstruction.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "    descriptor: np.ndarray (D, )\n",
    "        Complex descriptor with D features\n",
    "    \"\"\"\n",
    "    # Get number of samples\n",
    "    n_samples = len(descriptor)\n",
    "    n_mid = n_samples // 2\n",
    "    # Get intervals\n",
    "    n_rec = np.linspace(0, n_mid, n_mid+1).astype(int)\n",
    "\n",
    "    # Plot reconstruction\n",
    "    _, axes = plt.subplots(1, len(n_rec), figsize=(16, 5))\n",
    "    \n",
    "    for i, n in enumerate(n_rec):\n",
    "        # Create a local copy of the descriptor\n",
    "        d = descriptor.copy()\n",
    "        # Remove high frequencies (set to 0)\n",
    "        d = np.fft.fftshift(d)\n",
    "        d[:n_mid-n] = 0\n",
    "        d[n_mid+1+n:] = 0\n",
    "        # Reverse descriptors to coordinates\n",
    "        x, y = compute_reverse_descriptor(descriptor=np.fft.ifftshift(d), n_samples=n_samples)\n",
    "        # Plot contour with image overlay\n",
    "        axes[i].imshow(image, interpolation='nearest')\n",
    "        axes[i].scatter(x, y)\n",
    "        axes[i].plot(x, y)\n",
    "        axes[i].axis('off')\n",
    "        axes[i].set_title(\"N frequencies = {}\".format(1 + 2*n))\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "# Plot reconstruction\n",
    "plot_reconstruction(image=images_p_a[0], descriptor=feat_a[0, :])\n",
    "plot_reconstruction(image=images_p_b[0], descriptor=feat_b[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Invariance (6 pts)\n",
    "\n",
    "For the last part with Fourier descriptors, we will check for descriptors invariance. As seen in class, if handled properly Fourier descriptors can be invariant to translation, rotation, and scaling.\n",
    "\n",
    "* **Q1 (3 pts)**: Implement the functions `apply_rotation`, `apply_scaling`, and `apply_translate` to apply random rotation, scaling, and translation to input images. For scaling and translation, we recommend avoiding large values where the digits are cropped (out of frame). You can use the `random` package ([doc](https://numpy.org/doc/1.16/reference/routines.random.html)) from Numpy to generate random values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotation(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random rotation to input the image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    rotated: np.ndarray (28, 28)\n",
    "        Rotated source images\n",
    "    \"\"\"\n",
    "\n",
    "    rotated = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return rotated\n",
    "\n",
    "\n",
    "def apply_scaling(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random scaling to input image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    scaled: np.ndarray (28, 28)\n",
    "        Scaled source images\n",
    "    \"\"\"\n",
    "    \n",
    "    scaled = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def apply_translate(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Apply random x and y translation to input image\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    translated: np.ndarray (28, 28)\n",
    "        Translated source images\n",
    "    \"\"\"\n",
    "    \n",
    "    translated = np.zeros_like(img)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_tranformation(imgs: np.ndarray, func: Callable):\n",
    "    \"\"\"\n",
    "    Apply random transformation to a set of images\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "    func: Callable\n",
    "        Transformation function to apply to input images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    imgs_trans: np.ndarray (N, 28, 28)\n",
    "        Transformed images\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of images\n",
    "    n = len(imgs)\n",
    "    imgs_trans = np.zeros_like(imgs)\n",
    "\n",
    "    # Apply transformation\n",
    "    for i in range(n):\n",
    "        imgs_trans[i] = func(imgs[i])\n",
    "\n",
    "    return imgs_trans\n",
    "        \n",
    "\n",
    "def plot_transform(img, function, title):\n",
    "    \"\"\"\n",
    "    Plot random transformation for visualization purposes\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    image: np.ndarray (28, 28)\n",
    "        Source image\n",
    "    func: Callable\n",
    "        Transformation function to apply to input images\n",
    "    title: str\n",
    "        Title of the plot\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fix the number of examples to display \n",
    "    n = 10\n",
    "    _, axes = plt.subplots(1, n, figsize=(16, 2))\n",
    "\n",
    "    # Apply n random transformation on input images\n",
    "    for i in range(n):\n",
    "        trans = function(img=img)\n",
    "        axes[i].imshow(trans, interpolation='nearest')\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random rotations\n",
    "plot_transform(img=images_p_a[0], function=apply_rotation, title=\"Apply different rotation on image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random scaling\n",
    "plot_transform(img=images_p_a[0], function=apply_scaling, title=\"Apply different scaling on image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display random translations\n",
    "plot_transform(img=images_p_a[0], function=apply_translate, title=\"Apply different translation on image\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we implemented our transformation, we can test for feature invariance. To assess the robustness of the Fourier descriptors to image transformations, we compute the error between the original descriptors (before transformation) to the one after transformation. \n",
    "\n",
    "* **(1 pts)**: Complet the function `translation_invariant` to make the Fourier deciptor invariant to translation. Does the error decrease after treatment for translation invariance? Is it null, if not why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to translation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to translation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptors for translated images\n",
    "cnt_t_a = find_contour(apply_tranformation(imgs=images_p_a, func=apply_translate))\n",
    "feat_t_a = compute_descriptor_padding(contours=linear_interpolation(cnt_t_a))\n",
    "\n",
    "# Get invariant features\n",
    "a = translation_invariant(feat_a)\n",
    "b = translation_invariant(feat_t_a)\n",
    "\n",
    "# Compute errors\n",
    "error_no_corr = np.abs(feat_a - feat_t_a).mean()\n",
    "error_corr = np.abs(a - b).mean()\n",
    "# Print averaged error before\n",
    "print(\"Translation error: {:.2f}\".format(error_no_corr))\n",
    "# Print averaged error after\n",
    "print(\"Corrected translation error: {:.2f}\".format(error_corr))\n",
    "\n",
    "# plot features distribution\n",
    "plot_features(\n",
    "    features_a=feat_a,\n",
    "    features_b=feat_t_a,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-trans\",\n",
    "    title=\"Features w/o translation correction (error: {:.2f})\".format(error_no_corr),\n",
    ")\n",
    "\n",
    "plot_features(\n",
    "    features_a=a,\n",
    "    features_b=b,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-trans\",\n",
    "    title=\"Features w/ translation correction (error: {:.2f})\".format(error_corr),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(1 pts)**: Complet the function `rotation_invariant` to make the Fourier deciptor invariant to rotation. Does the error decrease after treatment for rotation invariance? Do you think that selecting specific rotation angles might help the error to decrease even lower?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotation_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to rotation.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to rotation of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptors for rotation imagescan\n",
    "cnt_r_a = find_contour(apply_tranformation(imgs=images_p_a, func=apply_rotation))\n",
    "feat_r_a = compute_descriptor_padding(contours=linear_interpolation(cnt_r_a))\n",
    "\n",
    "# Get invariant features\n",
    "a = rotation_invariant(feat_a)\n",
    "b = rotation_invariant(feat_r_a)\n",
    "\n",
    "# Compute errors\n",
    "error_no_corr = np.abs(feat_a - feat_r_a).mean()\n",
    "error_corr = np.abs(a - b).mean()\n",
    "# Print averaged error before\n",
    "print(\"Rotation error: {:.2f}\".format(error_no_corr))\n",
    "# Print averaged error after\n",
    "print(\"Corrected rotation error: {:.2f}\".format(error_corr))\n",
    "\n",
    "# plot features distribution\n",
    "plot_features(\n",
    "    features_a=feat_a,\n",
    "    features_b=feat_r_a,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-rot\",\n",
    "    title=\"Features w/o rotation correction (error: {:.2f})\".format(error_no_corr),\n",
    ")\n",
    "\n",
    "plot_features(\n",
    "    features_a=a,\n",
    "    features_b=b,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-rot\",\n",
    "    title=\"Features w/ rotation correction (error: {:.2f})\".format(error_corr),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **(1 pts)**: Complet the function `scaling_invariant` to make the Fourier deciptor invariant to scaling. Does the error decrease after treatment for scaling invariance? Is it null? Why?\n",
    "    * **Answer**: ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaling_invariant(features):\n",
    "    \"\"\"\n",
    "    Make input Fourier descriptors invariant to scaling.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    features: np.ndarray (N, D)\n",
    "        The Fourier descriptors of N images over D features.\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    features_inv: np.ndarray (N, K)\n",
    "        The Fourier descriptors invariant to scaling of N images \n",
    "        over K (K <= N) features.\n",
    "    \"\"\"\n",
    "\n",
    "    # Set default values\n",
    "    features_inv = np.zeros_like(features)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return features_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get descriptors for scaling images\n",
    "cnt_s_a = find_contour(apply_tranformation(imgs=images_p_a, func=apply_scaling))\n",
    "feat_s_a = compute_descriptor_padding(contours=linear_interpolation(cnt_s_a))\n",
    "\n",
    "# Get invariant features\n",
    "a = scaling_invariant(feat_a)\n",
    "b = scaling_invariant(feat_s_a)\n",
    "\n",
    "# Compute errors\n",
    "error_no_corr = np.abs(feat_a - feat_t_a).mean()\n",
    "error_corr = np.abs(a - b).mean()\n",
    "# Print averaged error before\n",
    "print(\"Scaling error: {:.2f}\".format(error_no_corr))\n",
    "# Print averaged error after\n",
    "print(\"Corrected scaling error: {:.2f}\".format(error_corr))\n",
    "\n",
    "# plot features distribution\n",
    "plot_features(\n",
    "    features_a=feat_a,\n",
    "    features_b=feat_s_a,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-sca\",\n",
    "    title=\"Features w/o scaling correction (error: {:.2f})\".format(error_no_corr),\n",
    ")\n",
    "\n",
    "plot_features(\n",
    "    features_a=a,\n",
    "    features_b=b,\n",
    "    label_a=label_a, label_b=str(label_a)+\"-sca\",\n",
    "    title=\"Features w/ scaling correction (error: {:.2f})\".format(error_corr),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# PART 3 - Other descriptors\n",
    "\n",
    "## 3.1 Distance map (5 pts)\n",
    "\n",
    "In this part, we will learn how to use a distance map as a feature descriptor. \n",
    "\n",
    "* **Q1 (1 pts)**: To compute a distance map we first need a reference pattern. Complete the function `reference_pattern`. The function takes as input a list of images and computes the reference pattern as the average of all shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference_pattern(imgs):\n",
    "    \"\"\"\n",
    "    Compute the reference pattern for a given set of images. The reference pattern \n",
    "    is estimated as the average of all images of the same pattern.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    pattern: np.ndarray (28, 28)\n",
    "        Thresholded reference pattern that is the average of all shapes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize pattern\n",
    "    pattern = np.zeros((imgs[0].shape[0], imgs[0].shape[1]))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "   \n",
    "    return pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reference_patterns(\n",
    "    pattern_a: np.ndarray, pattern_b: np.ndarray, title_a: str, title_b: str, map_a: np.ndarray = None, map_b: np.ndarray = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the reference patterns for the two patterns as well as distance maps if provided\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    pattern_a: np.ndarray (28, 28)\n",
    "        The first pattern to display\n",
    "    pattern_b: np.ndarray (28, 28)\n",
    "        The second pattern to display \n",
    "    title_a: str\n",
    "        Title of the first plot\n",
    "    title_a: str\n",
    "        Title of the first plot \n",
    "    map_a: np.ndarray (28, 28)\n",
    "        Distance map, If None, the map is not plotted\n",
    "    map_b: np.ndarray (28, 28)\n",
    "        Distance map 2nd, If None, the map is not plotted\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display results\n",
    "    fig, axes = plt.subplots(1, 4, figsize=(10, 2))\n",
    "    # Remove axes\n",
    "    [a.axis(\"off\") for a in axes]\n",
    "    # Show patterns\n",
    "    axes[0].imshow(pattern_a, interpolation=\"nearest\")\n",
    "    axes[0].set_title(title_a)\n",
    "    axes[1].imshow(pattern_b, interpolation=\"nearest\")\n",
    "    axes[1].set_title(title_b)\n",
    "    # Check if distance map exists\n",
    "    if map_a is not None:\n",
    "        pcm = axes[2].imshow(map_a, interpolation='nearest')\n",
    "        axes[2].set_title(title_a + \"\\n(Dist. map)\")\n",
    "        fig.colorbar(pcm, ax=axes[2])\n",
    "    if map_a is not None:\n",
    "        pcm = axes[3].imshow(map_b, interpolation='nearest')\n",
    "        axes[3].set_title(title_b + \" \\n(Dist. map)\")\n",
    "        fig.colorbar(pcm, ax=axes[3], label=\"distance to shape\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute the pattern for both digits\n",
    "pattern_a = reference_pattern(images_p_a)\n",
    "pattern_b = reference_pattern(images_p_b)\n",
    "\n",
    "plot_reference_patterns(\n",
    "    pattern_a=pattern_a,\n",
    "    pattern_b=pattern_b, \n",
    "    title_a=\"Reference pattern {}\".format(label_a),\n",
    "    title_b=\"Reference pattern {}\".format(label_b),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q2 (2 pts)**: The next part will be to compute a distance map from the generated pattern. By pre-computing the distance map we can speedup the inference time. Complete the function `compute_distance_map`. We expect the values of the map to represent the distances to the closest pattern contour. If needed, can take advantage of the functions you wrote in PART2 to detect and resample contours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance_map(pattern: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute the distance map for the given pattern. The values of the map are computed as \n",
    "    the distance to the closest pattern contour.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    pattern: np.ndarray (28, 28)\n",
    "        Pattern to process\n",
    "\n",
    "    Return\n",
    "    ------\n",
    "    distance_map: np.ndarray (28, 28)\n",
    "        Distance map where each entry is the distance to the closest pattern contour (shortest \n",
    "        distance to pattern)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize dummy values\n",
    "    distance_map = np.zeros_like(pattern)\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return distance_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_a=compute_distance_map(pattern_a)\n",
    "map_b=compute_distance_map(pattern_b)\n",
    "\n",
    "plot_reference_patterns(\n",
    "    pattern_a=pattern_a,\n",
    "    pattern_b=pattern_b, \n",
    "    map_a=map_a,\n",
    "    map_b=map_b,\n",
    "    title_a=\"Reference pattern {}\".format(label_a),\n",
    "    title_b=\"Reference pattern {}\".format(label_b),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Q3 (2 pts)**: For the last part, implement the function `compute_distance` that uses the precomputed distance map to evaluate the distance to all images. Note that for each image you should return the average of distances. As before, for each digit, you can compute the contour and estimate the point-to-point distance by evaluating the distance map at the xy contour coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distance(imgs, d_map):\n",
    "    \"\"\"\n",
    "    Compute the distances for each image with respect to the reference pattern using the precomputed \n",
    "    distance map. The final distance is the average of all distances from the image's contour points \n",
    "    to the reference pattern.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "    d_map: np.ndarray (28, 28)\n",
    "        The precomputed distance map where each entry is the distance to the closest pattern contour \n",
    "        (shortest distance to pattern)\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "    dist: np.ndarray (N, )\n",
    "        Averaged distance to pattern for each input image.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Default values\n",
    "    dist = np.zeros(len(imgs))\n",
    "\n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "    \n",
    "    return dist\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dmap_feautres(fa: np.ndarray, fb: np.ndarray, la: str, lb: str):\n",
    "    \"\"\"\n",
    "    Plot distance features for features A and B.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    fa: np.ndarray (2, N)\n",
    "        Features A. Represent distance to self (a->a) and to other (a->b)\n",
    "    fb: np.ndarray (2, N)\n",
    "        Features B. Represent distance to self (b->a) and to other (b->b)\n",
    "    la: str\n",
    "        Axis label for feature A\n",
    "    lb: str\n",
    "        Axis label for feature B\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define plot\n",
    "    _, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    # Plot features and display labels\n",
    "    ax.scatter(fa[0], fa[1], label=\"{}\".format(la), alpha=0.3)\n",
    "    ax.scatter(fb[0], fb[1], label=\"{}\".format(lb), alpha=0.3)\n",
    "    ax.set_xlabel(\"Distance to pattern {}\".format(la))\n",
    "    ax.set_ylabel(\"Distance to pattern {}\".format(lb))\n",
    "    plt.legend()\n",
    "\n",
    "\n",
    "# Get reference feature a->a, a->b, b->a, and b->b\n",
    "d_a2a = compute_distance(images_p_a, map_a)\n",
    "d_a2b = compute_distance(images_p_a, map_b)\n",
    "d_b2a = compute_distance(images_p_b, map_a)\n",
    "d_b2b = compute_distance(images_p_b, map_b)\n",
    "\n",
    "# Plot results\n",
    "plot_dmap_feautres(fa=np.stack([d_a2a, d_a2b]), fb=np.stack([d_b2a, d_b2b]), la=label_a, lb=label_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Others (3 pts)\n",
    "\n",
    "For the last part of the lab, we will compute other various visual features. \n",
    "\n",
    "* **Q1: (2 pts)**: Implement the function `compute_features` that estimates the digit's perimeter, area, compacity, and rectangularity. You can use the lecture to look for the definition of each feature. To help you with this task we strongly recommend using the `regionprops` ([doc](https://scikit-image.org/docs/stable/api/skimage.measure.html#skimage.measure.regionprops)) from scikit-image.\n",
    "* **Q2: (1pts)**: Which feature(s) seem(s) to show the best results to distinguish between both digits? (justify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(imgs: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute compacity for each input image.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    imgs: np.ndarray (N, 28, 28)\n",
    "        Source images\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "    f_peri: np.ndarray (N,)\n",
    "        Estimated perimeter length for each image\n",
    "    f_area: np.ndarray (N,)\n",
    "        Estimated area for each image\n",
    "    f_comp: np.ndarray (N,)\n",
    "        Estimated compacity for each image\n",
    "    f_rect: np.ndarray (N,)\n",
    "        Estimated rectangularity for each image\n",
    "    \"\"\"\n",
    "\n",
    "    f_peri = np.zeros(len(imgs))\n",
    "    f_area = np.zeros(len(imgs))\n",
    "    f_comp = np.zeros(len(imgs))\n",
    "    f_rect = np.zeros(len(imgs))\n",
    "    \n",
    "    # ------------------\n",
    "    # Your code here ... \n",
    "    # ------------------\n",
    "\n",
    "    return f_peri, f_area, f_comp, f_rect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_other_features(\n",
    "    f_peri: np.ndarray, f_area: np.ndarray, f_comp: np.ndarray, f_rect: np.ndarray, la: str, lb: str):\n",
    "    \"\"\"\n",
    "    Plot features distribution based on input features for the two digits.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    f_peri: np.ndarray (2, N)\n",
    "        Estimated perimeter length for both digits\n",
    "    f_area: np.ndarray (2, N)\n",
    "        Estimated area for both digits\n",
    "    f_comp: np.ndarray (2, N)\n",
    "        Estimated compacity for both digits\n",
    "    f_rect: np.ndarray (2, N)\n",
    "        Estimated rectangularity for both digits\n",
    "    la: str\n",
    "        Axis label for feature A\n",
    "    lb: str\n",
    "        Axis label for feature B\n",
    "    \"\"\"\n",
    "\n",
    "    # Define plot\n",
    "    _, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    # peri vs area\n",
    "    axes[0].scatter(f_peri[0], f_area[0], label=la, alpha=0.3)\n",
    "    axes[0].scatter(f_peri[1], f_area[1], label=lb, alpha=0.3)\n",
    "    axes[0].set_xlabel(\"Perimeter\")\n",
    "    axes[0].set_ylabel(\"Area\")\n",
    "    axes[0].set_title(\"Perimeter vs area\")\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # compacity vs rectangularity\n",
    "    axes[1].scatter(f_comp[0], f_rect[0], label=la, alpha=0.3)\n",
    "    axes[1].scatter(f_comp[1], f_rect[1], label=lb, alpha=0.3)\n",
    "    axes[1].set_xlabel(\"Compacity\")\n",
    "    axes[1].set_ylabel(\"Rectangularity\")\n",
    "    axes[1].set_title(\"Compacity vs rectangularity\")\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get features\n",
    "fa_peri, fa_area, fa_comp, fa_rect = compute_features(images_p_a)\n",
    "fb_peri, fb_area, fb_comp, fb_rect = compute_features(images_p_b)\n",
    "\n",
    "# Plot results\n",
    "plot_other_features(\n",
    "    f_peri=np.stack([fa_peri, fb_peri]),\n",
    "    f_area=np.stack([fa_area, fb_area]),\n",
    "    f_comp=np.stack([fa_comp, fb_comp]),\n",
    "    f_rect=np.stack([fa_rect, fb_rect]),\n",
    "    la=label_a, lb=label_b\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
